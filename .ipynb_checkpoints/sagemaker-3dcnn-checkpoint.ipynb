{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何将开源项目迁移到SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景\n",
    "每当我们计划使用机器学习解决一个业务问题时，一般会采取如下的方案：\n",
    "1. 当前业务问题，是否通过Saas的机器学习服务可以解决，比如图片的分类，可以考虑使用云服务提供商的SAAS服务，如AWS的Rekognition，通过API调用的方式可以快速实现自己的业务需求\n",
    "2. 当方案1无法满足需求时，比如需要更多的定制化方案时，可以考虑使用比如Sagemaker的内置算法来降低算法开发和调优的复杂度。\n",
    "3. 当方案2也无法满足需求时，比如需要自己来构建网络来解决业务问题，这个时候我们首先可以考虑在github上查找是否有已经经过验证的代码可以复用来降低复杂度和开发成本。同时为了更快速的搭建自己的机器学习环境以及实现更高效，更节约成本的方式来构建模型，此时可以考虑使用Sagemaker来搭建自己的机器学习环境实现这部分需求。\n",
    "基于此背景，该文将介绍如何将搜索到的开源项目迁移到SageMaker，实现更高效，更节约成本的具体实现。\n",
    "\n",
    "本文将从已下步骤进行介绍：\n",
    "1. 业务理解并搜索需要的开源项目\n",
    "2. Sagemaker notebook中运行代码\n",
    "3. 编写迁移到Sagemaker的notebook代码\n",
    "4. 预处理优化\n",
    "5. 训练优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 业务理解并搜索需要的开源项目\n",
    "本文已通过3DCNN解决视频分类的问题来举例，比如当前您当前有一批视频需要基于视频的内容进行分类，需要将视频分为喜剧，动物，通过github搜索到一个3dcnn的网络，下来我们介绍如何具体进行迁移。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker notebook中运行代码\n",
    "1. 在迁移之前首先我们将github上的代码下载到Sagemaker studio中[如何开始使用sagemaker studio](https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/studio.html)\n",
    "2. 通过Sagemaker studio 开始运行代码进行测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "import os \n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "将视频数据转换成np，下面的代码介绍如何将视频数据转换为np用于输入到网络中用于模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import *\n",
    "import codecs\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import videoto3d \n",
    "\n",
    "\n",
    "def loaddata(video_dir, vid3d, nclass, result_dir, color=False, skip=True):\n",
    "        files = os.listdir(video_dir)\n",
    "        X = []\n",
    "        labels = []\n",
    "        labellist = []\n",
    "\n",
    "        for filename in files:\n",
    "            if filename == '.DS_Store':\n",
    "                continue\n",
    "            if os.path.splitext(filename)[1] != \".mp4\":\n",
    "                continue\n",
    "            name = os.path.join(video_dir, filename)\n",
    "#             print('filename is11 ',filename)\n",
    "            label = vid3d.get_UCF_classname(filename)\n",
    "            if label not in labellist:\n",
    "                if len(labellist) >= nclass:\n",
    "                    continue\n",
    "                labellist.append(label)\n",
    "            labels.append(label)\n",
    "            X.append(vid3d.video3d(name, color=color, skip=skip))\n",
    "            \n",
    "        print('result_dir is ',result_dir)\n",
    "        fpath = result_dir + 'classes.txt'\n",
    "        fp = codecs.open(fpath,'a','utf-8')\n",
    "        print('labels length is ',len(labellist))\n",
    "        for i in range(len(labellist)):\n",
    "            fp.write('{}\\n'.format(labellist[i]))\n",
    "\n",
    "        for num, label in enumerate(labellist):\n",
    "            for i in range(len(labels)):\n",
    "                if label == labels[i]:\n",
    "                    labels[i] = num\n",
    "        if color:\n",
    "            return np.array(X).transpose((0, 2, 3, 4, 1)), labels\n",
    "        else:\n",
    "            return np.array(X).transpose((0, 2, 3, 1)), labels\n",
    "\n",
    "        \n",
    "def process():\n",
    "        nclass = 8\n",
    "        depth = 15\n",
    "        skip = False\n",
    "        color = True\n",
    "        img_rows, img_cols, frames = 32, 32, depth\n",
    "\n",
    "        channel = 3 if color else 1\n",
    "        fname_npz = 'np-datasets/train_data.npz'\n",
    "        output = 'default-output/'\n",
    "        videos = 'dataset/'\n",
    "\n",
    "        vid3d = videoto3d.Videoto3D(img_rows, img_cols, frames)\n",
    "        nb_classes = nclass\n",
    "        if os.path.exists(fname_npz):\n",
    "                loadeddata = np.load(fname_npz)\n",
    "                X, Y = loadeddata[\"X\"], loadeddata[\"Y\"]\n",
    "        else:\n",
    "                x, y = loaddata(videos, vid3d, nclass,\n",
    "                                output, color, skip)\n",
    "                X = x.reshape((x.shape[0], img_rows, img_cols, frames, channel))\n",
    "                Y = np_utils.to_categorical(y, nb_classes)\n",
    "\n",
    "                X = X.astype('float32')\n",
    "                np.savez(fname_npz, X=X, Y=Y)\n",
    "        print('Saved dataset to dataset.npz.')\n",
    "        print('X_shape:{}\\nY_shape:{}'.format(X.shape, Y.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename is dataset/Animal_1597729970539621.mp4\n",
      "filename is dataset/DIY_1597729970814458.mp4\n",
      "filename is dataset/Music_1597729971107432.mp4\n",
      "filename is dataset/Drama_1597729970847089.mp4\n",
      "result_dir is  default-output/\n",
      "labels length is  4\n",
      "Saved dataset to dataset.npz.\n",
      "X_shape:(4, 32, 32, 15, 3)\n",
      "Y_shape:(4, 8)\n"
     ]
    }
   ],
   "source": [
    "process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上代码会将视频数据（存储在dataset目录下的测试数据）通过opencv进行抽帧然后将其转换为numpy数组并最终将数据存储为.npz文件用于后续模型训练，预处理后的数据存储在default-output目录下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下来我们在notebook中测试训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2021-01-19 15:40:26.048071: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
      "2021-01-19 15:40:26.083300: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500000000 Hz\n",
      "2021-01-19 15:40:26.084625: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x466edb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-01-19 15:40:26.084693: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-01-19 15:40:26.088102: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 32, 32, 15, 32)    2624      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 15, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 32, 32, 15, 32)    27680     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 15, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 11, 11, 5, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 11, 11, 5, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 11, 11, 5, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 11, 11, 5, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 11, 11, 5, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 11, 11, 5, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 4, 4, 2, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 2, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 4104      \n",
      "=================================================================\n",
      "Total params: 1,249,512\n",
      "Trainable params: 1,249,512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3 samples, validate on 1 samples\n",
      "Epoch 1/3\n",
      "3/3 [==============================] - 2s 771ms/step - loss: 2.8657 - accuracy: 0.0000e+00 - val_loss: 1.4536 - val_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 2.6733 - accuracy: 0.0000e+00 - val_loss: 1.6088 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/3\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 1.9949 - accuracy: 0.0000e+00 - val_loss: 1.8208 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "! python3 sagemaker-3dcnn.py --batch 3 --data_dir np-datasets --epoch 3 --output default-output  --nclass 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当前使用测试数据可以跑通网络，由于数据仅仅用于测试这里针对模型的效果暂时不做处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 编写迁移到Sagemaker的notebook代码\n",
    " 下来我们将介绍如何将如上运行通过的代码迁移到Sagemaker，针对迁移到Sagemaker有如下两种方案：\n",
    " 1. BYOS（Bring Your Own Script），也就是说直接使用现有的网络代码并迁移到sagemaker\n",
    " 2. BYOC（Bring Your Own Container），也就是说将现有代码构建程自定义Docker镜像的方式迁移到Sagemaker\n",
    " 一般情况下建议使用BYOS的方案，该方案更加简单，当BYOS方案无法满足需求时比如需要使用您当前的环境代码和依赖直接迁移到Sagemaker该方案相较于BYOS需要自己构建镜像。\n",
    " 本文将介绍如何使用BYOS进行迁移。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据准备\n",
    "当使用Sagemaker进行数据预处理或者模型训练时，建议将数据上传到S3，这样便于针对大数据量场景下的数据预处理和模型训练，下面我们将预处理后的数据上传到S3。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-cn-northwest-1-462130072016/np-datasets'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将np-datasets目录下的np文件上传到sagemaker-studio默认桶下的np-datasets文件夹下\n",
    "inputs = sagemaker.Session().upload_data(path='np-datasets', key_prefix='np-datasets')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打开S3控制台查看如上输出路径可以看到预处理后的数据已经上传到S3中了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数设置\n",
    "在本地运行的时候网络的参数是通过 python命令行传入的，当迁移到Sagemaker后可以通过参数的方式设置超参数，然后传递给Sagemaker进行使用。\n",
    "参数说明：\n",
    "1. data_dir：指定训练数据从S3下载后存储到训练机器上的路径\n",
    "2. output：指定模型训练完成后模型存储在训练机器上的路径\n",
    "3. epoch/batch/nclass：此部分参数是原有网络中使用的参数，提取出来通过超参数传递给网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'epoch': 3, \n",
    "                   'data_dir': '/opt/ml/input/data/training',\n",
    "                   'batch': 3, \n",
    "                   'nclass': 8,\n",
    "                   'output': '/opt/ml/output',\n",
    "                  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码修改\n",
    "由于Sagemaker会自动通过S3下载训练数据到模型训练的机器，然后使用下载的数据进行模型训练，同时也会自动将训练好的模型上传到S3中，因此我们需要对原有的代码进行调整，添加如下参数：\n",
    "1. model_dir：用于指定模型的S3存储路径，如果不设置的话默认会存储到Sagemaker studio默认创建的桶中\n",
    "2. data_dir： 用于指定模型训练的训练数据存储路径，路径对应fit方法的训练输入路径参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* parser.add_argument('--model_dir', type=str)\n",
    "* parser.add_argument('--data_dir', type=str, default=os.environ.get('SM_CHANNEL_TRAINING'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Sagemaker Python SDK API 实现模型的训练\n",
    "Sgamaker的有两种方式：\n",
    "1. Sagemaker low level的API 也就是Sgaemaker 的API，这个是针对Sagemaker服务开放的API支持多种语言如Python java具体参考[Sagamker API](https://docs.amazonaws.cn/sagemaker/latest/dg/api-and-sdk-reference.html)\n",
    "2. Sagemaker high level的API 也就是[Sagemaker的Python SDK](https://sagemaker.readthedocs.io/en/stable/)\n",
    "使用Sagemaker high level的API更加简单和强大，一般会建议使用该方案。下来我们使用Sagemaker Python SDK提供的TensoFlow 评估器(Estimator)来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = '727897471807.dkr.ecr.cn-northwest-1.amazonaws.com/tensorflow-training:1.15.4-gpu-py37-cu100-ubuntu18.04'\n",
    "## Deep Learning Container 说明地址：https://github.com/aws/deep-learning-containers\n",
    "## DLC 地址 ：https://github.com/aws/deep-learning-containers/blob/master/available_images.md 可用列表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point='sagemaker-3dcnn.py',\n",
    "#                        image_uri = image_uri,\n",
    "                       train_instance_type='ml.c4.xlarge',\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       framework_version='2.3.1',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-20 01:50:55 Starting - Starting the training job...\n",
      "2021-01-20 01:50:57 Starting - Launching requested ML instances..."
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.1 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws-cn:sagemaker:cn-northwest-1:390780980154:image/tensorflow-2.1-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
